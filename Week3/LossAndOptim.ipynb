{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LossAndOptim.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPvjYP8e2az910yqg614qpF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### Loss function\n","\n","Loss는 머신러닝이 학습을 얼마나 잘 하고 있는지 확인하는 지표입니다. 모델이 예측한 결과와 실제 결과가 얼마나 차이가 나는지를 계산하여 이를 최소화 하는 방향으로 학습을 진행합니다.\n","\n","회귀 : 예측 값이 숫자값일때\n","- MSE(Mean square error)\n","- MAE(Mean absolute error)\n","\n","분류 : 예측 값이 범주값일떄\n","- Cross Entropy"],"metadata":{"id":"_RW1HVacg00C"}},{"cell_type":"code","source":["import numpy as np\n","def mae(y_true, y_pred):\n","    return np.abs(y_true - y_pred).mean()\n","\n","def mse(y_true, y_pred):\n","    return np.square(y_true - y_pred).mean()"],"metadata":{"id":"tyo9_PrthJAA","executionInfo":{"status":"ok","timestamp":1658129619832,"user_tz":-540,"elapsed":2,"user":{"displayName":"‍조희승[ 대학원석·박사통합과정재학 / 인공지능학과 ]","userId":"05467509791062225760"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["y_true = np.array([1,2,3,4,5,6])\n","y_pred = np.array([1,3,4,4,5,9])\n","print(\"MAE: \",mae(y_true,y_pred))\n","print(\"MSE: \",mse(y_true,y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WPWbgoorhJf3","executionInfo":{"status":"ok","timestamp":1658129664033,"user_tz":-540,"elapsed":9,"user":{"displayName":"‍조희승[ 대학원석·박사통합과정재학 / 인공지능학과 ]","userId":"05467509791062225760"}},"outputId":"a98cd7f0-4c16-472a-ceae-9b1d31a8f8bc"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["MAE:  0.8333333333333334\n","MSE:  1.8333333333333333\n"]}]},{"cell_type":"markdown","source":["#### Cross Entropy\n","\n","![image](http://androidkt.com/wp-content/uploads/2021/05/Selection_098.png)"],"metadata":{"id":"b88wYeFrPFj-"}},{"cell_type":"code","source":["y_pred = np.array([0.001, 0.9, 0.001, 0.098])\n","y_true = np.array([0, 0, 0, 1])\n","\n","def cross_entropy_error(y_true,y_pred):\n","    eps = 1e-7\n","    return -np.sum(np.log(y_pred+eps) * y_true)\n","cross_entropy_error(y_true,y_pred)  "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"81GhDMmWO2SX","executionInfo":{"status":"ok","timestamp":1658129886107,"user_tz":-540,"elapsed":352,"user":{"displayName":"‍조희승[ 대학원석·박사통합과정재학 / 인공지능학과 ]","userId":"05467509791062225760"}},"outputId":"c56738e1-b143-4ff4-a929-d69dff29b832"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2.3227867799039226"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["### Optimization\n","\n","Optimization이란 모델이 가지고 있는 파라미터들을 갱신해나가는 알고리즘입니다.\n","앞선 머신러닝에서 w와 b의 값이 변하는 것을 확인할 수 있었는데요, 이는 MSE의 미분 값을 토대로 파라미터가 학습해나가는 과정입니다. 이 때 learning rate라는 변수를 통해 학습의 정도를 조절하여 모델이 최적화되는 과정을 확인할 수 있었습니다.\n","\n","머신러닝 및 딥러닝에서 자주 사용하는 optimizer는 다음과 같습니다.\n","\n","- Gradient Descent\n","- Momentum\n","- Root Mean Square Propatation(RMSProp)\n","- Adam"],"metadata":{"id":"gjRv-GudhLxK"}},{"cell_type":"markdown","source":["#### Gradient Descent\n","\n","파라미터가 가지고 있는 미분값을 토대로 갱신해나가는 최적화 알고리즘입니다.\n","\n","- W=W−αdW\n","- b=b−αdb\n","- α : learning rate\n","\n","![image](https://t1.daumcdn.net/cfile/tistory/994F59375D818E5E36)"],"metadata":{"id":"ohs1nRyuhNGH"}},{"cell_type":"markdown","source":["#### Gradient with momentum\n","Gradient descent에 momentum 기법을 적용한 알고리즘입니다.\n","momentum가 기울기의 지수 가중 편중치를 산출하여 weight를 업데이트하게 해줌으로써 속도가 더욱 빨라지고 diverge하거나 local minimum에 빠지는 것을 방지해줍니다.\n","\n","$v_{dW}=β*v_{dW}+(1−β)d_{W}$\n","\n","$v_{db} =β*v_{db} +(1−β)*db$\n","\n","$W=W−α*v_{dW}$\n","\n","$​b=b−α*v_{db}$\n"],"metadata":{"id":"iIJRwuSPU3Cz"}},{"cell_type":"markdown","source":["#### RMSprop\n","\n","RMSprop 역시 기울기 강하의 속도를 증가시킵니다.\n","\n","$S_{dW} =β_2S_{dW} +(1−β_2)dW^2$\n","\n","$S_{db} =β_2S_{db} +(1−β_2)db^2$\n"," \n","$W=W−αdW/\\sqrt{S_{dW}+ϵ}$\n","\n","$​b=b−αdW/\\sqrt{S_{db}+ϵ}$\n","​\n"],"metadata":{"id":"d-lbwoYuWqRB"}},{"cell_type":"markdown","source":["#### Adam\n","\n","모멘텀과 RMSProp을 섞은 최적화 알고리즘입니다.\n","\n","Momentum의 $\\beta_1$와 RMSprop의 $\\beta_2$를 각각 파라미터로 받아 경사하강의 속도를 조정해줍니다.\n","\n","![image](https://blog.kakaocdn.net/dn/2gQ6q/btqX0pphrUe/2wAOBFAAb4TIaamm3Xoj70/img.png)"],"metadata":{"id":"Tg872J94WBBR"}},{"cell_type":"markdown","source":["### Backpropagation\n","\n","여기서 문제는, 어떻게 dW와 db를 구할 수 있을까?\n","모델이 input으로부터 진행하여 output을 내는 과정을 Forward propagation이라고 한다면,\n","반대로 output으로부터 input을 역방향을 통하여 미분값들을 구하는 과정을 역전파(backpropagation)라고 합니다.\n","\n","Input과 output, 그리고 loss가 계산된다면, 역전파는 이들의 미분값을 chain rule을 통해 곱해가면서 파라미터가 가진 미분값을 구하게 됩니다. \n","\n","![image](https://images.velog.io/images/fbdp1202/post/e97416eb-12e7-4f28-88d9-04ddf746f74c/cs231n-04-007-Backpropagation_fig.png)\n"],"metadata":{"id":"aSVjdOLMhJxz"}}]}